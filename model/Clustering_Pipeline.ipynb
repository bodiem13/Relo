{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell needs to be run twice, so the plots will display\n",
    "\n",
    "import time\n",
    "import itertools\n",
    "from itertools import cycle\n",
    "from statistics import mean\n",
    "import pickle\n",
    "\n",
    "# Data Handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import linalg\n",
    "\n",
    "from sklearn.preprocessing import scale, StandardScaler, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Dimensionality Reduction\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Clustering\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans, SpectralClustering, AgglomerativeClustering, Birch\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
    "from kemlglearn.cluster.consensus import SimpleConsensusClustering\n",
    "import openensembles as oe\n",
    "\n",
    "# Evaluation\n",
    "from pyclustertend import hopkins, vat, ivat\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "from kneed import KneeLocator\n",
    "import gapstat_rs\n",
    "from gap_statistic import OptimalK\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import silhouette_score\n",
    "from amltlearn.metrics.cluster import calinski_harabasz_score, davies_bouldin_score\n",
    "\n",
    "# Visualization\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib import animation\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import mpl_toolkits.mplot3d.axes3d as p3\n",
    "import seaborn as sns\n",
    "from pylab import *\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "%matplotlib inline \n",
    "sns.set_context('poster')\n",
    "sns.set_color_codes()\n",
    "plot_kwds = {'alpha' : 0.25, 's' : 80, 'linewidths':0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_corr(df):\n",
    "    corr = df.corr()\n",
    "    mask = np.zeros_like(corr, dtype=np.bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "    f, ax = plt.subplots(figsize=(11, 9))\n",
    "    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "                square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_internal_validity(max_clusters, internal_measure, name):\n",
    "    fig = plt.figure(figsize=(15,5))\n",
    "    plt.style.use(\"fivethirtyeight\")\n",
    "    plt.plot(range(2, max_clusters+1), internal_measure)\n",
    "    plt.xticks(range(2, max_clusters+1))\n",
    "    plt.xlabel(\"Number of Clusters\")\n",
    "    plt.ylabel(name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_pca(n_components, data, labels):\n",
    "    matrix = PCA(n_components=n_components, svd_solver='full').fit_transform(data)\n",
    "    \n",
    "    names = ['x', 'y', 'z']\n",
    "    df_matrix = pd.DataFrame(matrix)\n",
    "    df_matrix.rename({i:names[i] for i in range(n_components)}, axis=1, inplace=True)\n",
    "    df_matrix['labels'] = labels\n",
    "    \n",
    "    return df_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_tsne(n_components, data, labels):\n",
    "    pca = PCA(.95, svd_solver='full') \n",
    "    X_pca = pca.fit_transform(data)\n",
    "    \n",
    "    tsne = TSNE(n_components=n_components, verbose=0, perplexity=40, n_iter=300)\n",
    "    matrix = tsne.fit_transform(X_pca)\n",
    "    \n",
    "    names = ['x', 'y', 'z']\n",
    "    df_matrix = pd.DataFrame(matrix)\n",
    "    df_matrix.rename({i:names[i] for i in range(n_components)}, axis=1, inplace=True)\n",
    "    df_matrix['labels'] = labels\n",
    "    \n",
    "    return df_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2d(df, model, centers=False):\n",
    "    colors = plt.cm.Spectral(np.linspace(0, 1, len(df.labels.unique())))\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    plt.style.use(\"fivethirtyeight\")\n",
    "    \n",
    "    for color, label in zip(colors, df.labels.unique()):\n",
    "    \n",
    "        tempdf = df[df.labels == label]\n",
    "        plt.scatter(tempdf.x, tempdf.y, c=color)\n",
    "    \n",
    "    if centers:\n",
    "        plt.scatter(model.cluster_centers_[:,0], model.cluster_centers_[:, 1], c='r', s=500, alpha=0.7, )\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3d(df):\n",
    "    def update(num):\n",
    "        ax.view_init(200, num)\n",
    "\n",
    "    N=360\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    plt.style.use(\"fivethirtyeight\")\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(df['x'], df['y'], df['z'], c=df[\"labels\"],\n",
    "               s=6, depthshade=True, cmap='Paired')\n",
    "    ax.set_zlim(-15, 25)\n",
    "    ax.set_xlim(-20, 20)\n",
    "    plt.tight_layout()\n",
    "    ani = animation.FuncAnimation(fig, update, N, blit=False, interval=50)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Tendency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the raw Census and Amenities data\n",
    "df_viz = pd.read_pickle(r'Final_Visualization_Input_Data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only retain the columns that will be utilized in the clustering models\n",
    "column_list = ['GEOID', 'GEO_ID', 'NAME', 'SUM_IND_Child_Dep_Ratio',\n",
    "       'SUM_IND_Old_Age_Dep_Ratio', 'SUM_IND_Med_Age', 'MED_INC_25_PLUS_Tot',\n",
    "       'PERC_BEL_POV', 'LAB_FRC_POP_20_to_64', 'UNEMP_RATE_POP_20_to_64',\n",
    "       'HH_FAM_Avg_Fam_Size', 'HH_TOT_1_Unit_Stuct',\n",
    "       'HH_TOT_2_Plus_Unit_Struct', 'HH_TOT_Owner', 'RACE_One_Race_White',\n",
    "       'RACE_One_Race_Black', 'RACE_One_Race_Asian',\n",
    "       'RACE_One_Race_Hawaiian_PacIsl', 'RACE_One_Race_Other', 'RACE_Tot_Hisp',\n",
    "       'HOUSE_Tot_House_Units', 'EDU_25_PLUS_w_HS_or_GED',\n",
    "       'EDU_25_PLUS_w_Bachelors_Plus',\n",
    "       'CIV_EMP_POP_16_PLUS_GRP_Mngmt_Bus_Sci_Art',\n",
    "       'CIV_EMP_POP_16_PLUS_GRP_Srv', 'CIV_EMP_POP_16_PLUS_GRP_Sls_Office',\n",
    "       'CIV_EMP_POP_16_PLUS_GRP_NatRes_Constr_Maint',\n",
    "       'CIV_EMP_POP_16_PLUS_GRP_Prod_Trans_MatMov', 'WT_N_GROCERY_DIST_25',\n",
    "       'WT_N_GYMS_DIST_25', 'WT_N_HARDWARE_DIST_25', 'WT_N_PARKS_DIST_25',\n",
    "       'WT_N_MEDICAL_DIST_25']\n",
    "\n",
    "df_viz = df_viz[column_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the data values from the feature columns and scale the data\n",
    "X_raw = df_viz[df_viz.columns[~df_viz.columns.isin([\"GEOID\",\"GEO_ID\",\"NAME\"])]].values\n",
    "\n",
    "X_scale = scale(X_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hopkins Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hopkins Test on Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hopkins(X_scale, X_scale.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hopkins Test on Data Subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Hopkins Test is computationally expensive, especially on large datasets\n",
    "# So, we will create four equally sized dataframes by randomly separating the rows\n",
    "# Then, we'll run the Hopkins Test on each of these data subsets, and average the results\n",
    "# We will run this sequence a total of K times to get the best estimate of the dataset's \"cluster tendency\"\n",
    "\n",
    "# The number of iterations that will be run\n",
    "K = 10\n",
    "\n",
    "# Create a dictionary to store the results of each iteration\n",
    "hopkins_test_results = {}\n",
    "\n",
    "for k in range(K):\n",
    "    # Store the results of the kth iteration in a list\n",
    "    hopkins_test_results[k] = []\n",
    "    \n",
    "    # Randomly separate the original dataframe into four equally sized dataframe subsets\n",
    "    df_dict = {}\n",
    "    df_i = df_viz[df_viz.columns[~df_viz.columns.isin([\"GEO_ID\",\"NAME\"])]].copy()\n",
    "\n",
    "    for i in range(4):\n",
    "        if i < 3:\n",
    "            # Create a dataframe subset and store it in the appropriate dictionary\n",
    "            df_sample = df_i.sample(n=int(df.shape[0]/4), axis=0).reset_index(drop=True)\n",
    "            df_dict[i] = df_sample\n",
    "            \n",
    "            # Remove the selected rows from the primary dataframe, so they aren't selected again\n",
    "            a = df_sample['GEOID'].values\n",
    "            df_i = df_i[~df_i['GEOID'].isin(a)]\n",
    "        else:\n",
    "            # If it's the last iteration, simply retain the remaining dataframe subset\n",
    "            df_dict[i] = df_i\n",
    "    \n",
    "    # For each dataframe subset, run the Hopkins Test and store the results in the appropriate list\n",
    "    for i in df_dict.keys():\n",
    "        # Retrieve the dataframe subset and drop any null values\n",
    "        df_i = df_dict[i]\n",
    "        df_i.dropna(axis=1, how='any', inplace=True)\n",
    "        \n",
    "        # Retrieve the data values and scale the data\n",
    "        X_raw_i = df_i[df_i.columns[~df_i.columns.isin([\"GEOID\"])]].values\n",
    "        X_scale_i = scale(X_raw_i)\n",
    "        \n",
    "        # Calculate the Hopkins Statistic and store the results\n",
    "        results = hopkins(X_scale_i, X_scale_i.shape[0])\n",
    "        hopkins_test_results[k].append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the \"results\" dictionary and take the average of the Hopkins Statistic for each iteration\n",
    "\n",
    "for k in hopkins_test_results.keys():\n",
    "    string = \"The average Hopkins Statistic of the {} iteration: {}\".format(k, mean(hopkins_test_results[k]))\n",
    "    print(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAT on Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vat(X_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ivat algorithm is a improved version of the vat algorithm which produce more precise images at the cost of a heavier computing cost\n",
    "#ivat(X_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAT on Data Subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly separate the original dataframe into four equally sized dataframe subsets\n",
    "df_dict = {}\n",
    "df_i = df_viz[df_viz.columns[~df_viz.columns.isin([\"GEO_ID\",\"NAME\"])]].copy()\n",
    "\n",
    "for i in range(4):\n",
    "    if i < 3:\n",
    "        # Create a dataframe subset and store it in the appropriate dictionary\n",
    "        df_sample = df_i.sample(n=int(df.shape[0]/4), axis=0).reset_index(drop=True)\n",
    "        df_dict[i] = df_sample\n",
    "\n",
    "        # Remove the selected rows from the primary dataframe, so they aren't selected again\n",
    "        a = df_sample['GEOID'].values\n",
    "        df_i = df_i[~df_i['GEOID'].isin(a)]\n",
    "    else:\n",
    "        # If it's the last iteration, simply retain the remaining dataframe subset\n",
    "        df_dict[i] = df_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each dataframe subset, run the Hopkins Test and store the results in the appropriate list\n",
    "for i in df_dict.keys():\n",
    "    # Retrieve the dataframe subset and drop any null values\n",
    "    df_i = df_dict[i]\n",
    "    df_i.dropna(axis=1, how='any', inplace=True)\n",
    "\n",
    "    # Retrieve the data values and scale the data\n",
    "    X_raw_i = df_i[df_i.columns[~df_i.columns.isin([\"GEOID\"])]].values\n",
    "    X_scale_i = scale(X_raw_i)\n",
    "\n",
    "    # VAT\n",
    "    #vat(X_scale_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each dataframe subset, run the Hopkins Test and store the results in the appropriate list\n",
    "for i in df_dict.keys():\n",
    "    # Retrieve the dataframe subset and drop any null values\n",
    "    df_i = df_dict[i]\n",
    "    df_i.dropna(axis=1, how='any', inplace=True)\n",
    "\n",
    "    # Retrieve the data values and scale the data\n",
    "    X_raw_i = df_i[df_i.columns[~df_i.columns.isin([\"GEOID\"])]].values\n",
    "    X_scale_i = scale(X_raw_i)\n",
    "\n",
    "    # iVAT\n",
    "    #ivat(X_scale_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned and scaled Census and Amenities data\n",
    "df_model = pd.read_pickle(r'Final_Clustering_Input_Data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a subset of the data\n",
    "#df = df.sample(n=int(df.shape[0]/4), axis=0).reset_index(drop=True)\n",
    "\n",
    "# Gather all the values from the feature columns of the dataframe\n",
    "X = df_model[df_model.columns[~df_model.columns.isin([\"GEOID\",\"GEO_ID\",\"NAME\"])]].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = df_model[df_model.columns[~df_model.columns.isin([\"GEOID\",\"GEO_ID\",\"NAME\"])]]\n",
    "\n",
    "plot_corr(df_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trisurface Plot for Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating correlation data \n",
    "df_corr = df_model[df_model.columns[~df_model.columns.isin([\"GEOID\",\"GEO_ID\",\"NAME\"])]].corr() \n",
    "df_corr.index = range(0, len(df_corr)) \n",
    "df_corr.rename(columns = dict(zip(df_corr.columns, df_corr.index)), inplace = True) \n",
    "df_corr = df_corr.astype(object) \n",
    "  \n",
    "# Generating coordinates with corresponding correlation values \n",
    "for i in range(0, len(df_corr)): \n",
    "    for j in range(0, len(df_corr)): \n",
    "        if i != j: \n",
    "            df_corr.iloc[i, j] = (i, j, df_corr.iloc[i, j]) \n",
    "        else: \n",
    "            df_corr.iloc[i, j] = (i, j, 0) \n",
    "\n",
    "df_list = [] \n",
    "  \n",
    "# flattening dataframe values \n",
    "for sub_list in df_corr.values: \n",
    "    df_list.extend(sub_list) \n",
    "\n",
    "# converting list of tuples into trivariate dataframe \n",
    "plot_df = pd.DataFrame(df_list) \n",
    "  \n",
    "fig = plt.figure() \n",
    "ax = Axes3D(fig) \n",
    "  \n",
    "# plotting 3D trisurface plot \n",
    "ax.plot_trisurf(plot_df[0], plot_df[1], plot_df[2], cmap = cm.jet, linewidth = 0.2) \n",
    "  \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the Coordinates of the Right-Singular Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape\n",
    "print(\"X:\", X.shape)\n",
    "\n",
    "# Find the minimum dimension\n",
    "s = min(X.shape)\n",
    "print(\"s = min({}, {}) == {}\".format(X.shape[0], X.shape[1], s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Singular Value Decomposition\n",
    "U, Sigma, VT = np.linalg.svd(X, full_matrices=False)\n",
    "\n",
    "print(\"U:\", U.shape)\n",
    "print(\"Sigma:\", Sigma.shape)\n",
    "print(\"VT:\", VT.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the coordinates of the top two (k_approx, below) right-singular vectors\n",
    "m, d = X.shape\n",
    "k_approx = 2\n",
    "assert k_approx <= s\n",
    "\n",
    "# Plot the components of the first k_approx=2 singular vectors\n",
    "fig, axs = plt.subplots(1, k_approx, sharex=True, sharey=True,\n",
    "                        figsize=(10*k_approx, 10))\n",
    "for k in range(k_approx):\n",
    "    axs[k].scatter(np.arange(min(m, d)), VT[k, :].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D Scatter Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2, svd_solver='full')\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "print(\"Explained variation per principal component: {} \\n\".format(pca.explained_variance_ratio_))\n",
    "print(\"Cumulative explained variation of the principal components: {}\".format(np.sum(pca.explained_variance_ratio_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's possible to take the original data and project it onto the 2-dimensional subspace defined by the first two right-singular vectors.\n",
    "df_pca = pd.DataFrame(data=X_pca, columns=[\"component_1\", \"component_2\"])\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "plt.scatter(df_pca[\"component_1\"], df_pca[\"component_2\"])\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.axis(\"square\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D Scatter Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3, svd_solver='full')\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "print(\"Explained variation per principal component: {} \\n\".format(pca.explained_variance_ratio_))\n",
    "\n",
    "print(\"Cumulative explained variation of the principal components: {}\".format(np.sum(pca.explained_variance_ratio_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], zs=X_pca[:, 2], depthshade=False, s=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-distributed Stochastic Neighbor Embedding (t-SNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scikit-learn chooses the minimum number of principal components such that 95% of the variance is retained.\n",
    "pca = PCA(.95, svd_solver=\"full\") \n",
    "\n",
    "# Select the number of components for PCA\n",
    "#pca = PCA(n_components=50, svd_solver='full')\n",
    "\n",
    "# Transform the data\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# The number of total components\n",
    "print(\"The total number of principal components: {} \\n\".format(len(pca.explained_variance_ratio_)))\n",
    "\n",
    "print(\"Explained variation per principal component: {} \\n\".format(pca.explained_variance_ratio_))\n",
    "\n",
    "# The explained variance tells you how much information (variance) can be attributed to each of the principal components.\n",
    "print(\"Cumulative explained variation for the principal components: {}\".format(np.sum(pca.explained_variance_ratio_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D Scatter Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "tsne = TSNE(n_components=2, verbose=0, perplexity=40, n_iter=300)\n",
    "X_pca_tsne = tsne.fit_transform(X_pca)\n",
    "\n",
    "print(\"Time elapsed: {} seconds\".format(time.time()-time_start))\n",
    "\n",
    "df_pca_tsne = pd.DataFrame(data=X_pca_tsne, columns=[\"component_1\", \"component_2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "sns.scatterplot(\n",
    "    x=\"component_1\", y=\"component_2\",\n",
    "    palette=sns.color_palette(\"hls\", 10),\n",
    "    data=df_pca_tsne,\n",
    "    legend=\"full\",\n",
    "    alpha=0.3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "plt.scatter(df_pca_tsne[\"component_1\"], df_pca_tsne[\"component_2\"])\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.axis(\"square\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results of the different clustering methods in a dictionary\n",
    "cluster_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitioning Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the appropriate Dimensionality Reduction technique and number of components \n",
    "\n",
    "# scikit-learn chooses the minimum number of principal components such that 95% of the variance is retained.\n",
    "#pca = PCA(.95, svd_solver=\"full\") \n",
    "\n",
    "# Select the number of components for PCA\n",
    "pca = PCA(n_components=2, svd_solver='full')\n",
    "\n",
    "# Transform the data\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Select X_pca or X_pca_tsne\n",
    "X_cluster = X_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the maximum number of clusters to test\n",
    "max_clusters = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Characteristics\n",
    "\n",
    "*Parameters:* number of clusters\n",
    "\n",
    "*Scalability:* Very large n_smaples\n",
    "\n",
    "*Usecase:* General-purpose, even cluster size, flat geometry, not too many clusters\n",
    "\n",
    "*Geometry (metric used):* Distances between points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose the Appropriate Number of Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "# Calculate the internal criteria that measure properties expected in a good clustering, compact and well-separated groups\n",
    "kmeans_kwargs = {\n",
    "    \"init\": \"random\",\n",
    "    \"n_init\": 10,\n",
    "    \"max_iter\": 300,\n",
    "    \"random_state\": 42,\n",
    "}\n",
    "\n",
    "# Create lists that hold the values of the different internal metrics for each k\n",
    "sse = []\n",
    "silhouette = []\n",
    "calinski_harabasz = []\n",
    "davies_bouldin = []\n",
    "\n",
    "for k in range(2, max_clusters+1):\n",
    "    model = KMeans(n_clusters=k, **kmeans_kwargs)\n",
    "    model.fit(X_cluster)\n",
    "    labels = model.fit_predict(X_cluster)\n",
    "    \n",
    "    sse.append(model.inertia_)\n",
    "    \n",
    "    score = silhouette_score(X_cluster, labels)\n",
    "    silhouette.append(score)\n",
    "    \n",
    "    score = calinski_harabasz_score(X_cluster, labels)\n",
    "    calinski_harabasz.append(score)\n",
    "    \n",
    "    score = davies_bouldin_score(X_cluster, labels)\n",
    "    davies_bouldin.append(score)\n",
    "    \n",
    "print(\"Time elapsed: {} seconds\".format(time.time()-time_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Elbow Method - SSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SSE: Quadratic error/Distorsion (k-means); the lower number the better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_internal_validity(max_clusters, sse, \"SSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl = KneeLocator(\n",
    "    range(2, max_clusters+1), sse, curve=\"convex\", direction=\"decreasing\"\n",
    ")\n",
    "\n",
    "n_clusters = kl.elbow\n",
    "n_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Internal Validity Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Silhouette Coefficient: maximum class spread/variance; the higher number the better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_internal_validity(max_clusters, silhouette, \"Silhouette\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calinski-Harabasz Index: interclass-intraclass distance ratio; the higher the number the better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_internal_validity(max_clusters, calinski_harabasz, \"Calinski Harabasz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Davies-Bouldin Criteria: maximum interclass-intraclass distance ratio; the lower the number the better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_internal_validity(max_clusters, davies_bouldin, \"Davies Bouldin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The Gap Statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an \"optimalK\" object\n",
    "optimalK = OptimalK(parallel_backend='rust')\n",
    "optimalK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call \"optimalK\" with a list of clusters to fit to\n",
    "n_clusters = optimalK(X_cluster, cluster_array=np.arange(1, max_clusters+1))\n",
    "print('Optimal clusters: ', n_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A DataFrame of gap values with each passed cluster count is now available\n",
    "#optimalK.gap_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the n_clusters against their gap values\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "plt.plot(optimalK.gap_df.n_clusters, optimalK.gap_df.gap_value, linewidth=3)\n",
    "plt.scatter(optimalK.gap_df[optimalK.gap_df.n_clusters == n_clusters].n_clusters,\n",
    "            optimalK.gap_df[optimalK.gap_df.n_clusters == n_clusters].gap_value, s=250, c='r')\n",
    "plt.grid(True)\n",
    "plt.xlabel('Cluster Count')\n",
    "plt.ylabel('Gap Value')\n",
    "plt.title('Gap Values by Cluster Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model with Optimal Number of Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the optimal number of clusters\n",
    "optimal_clusters = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "# Instantiate the k-means algorithm\n",
    "kmeans_kwargs = {\n",
    "    \"init\": \"k-means++\",\n",
    "    \"n_clusters\": optimal_clusters,\n",
    "    \"n_init\": 50,\n",
    "    \"max_iter\": 500,\n",
    "    \"algorithm\": \"full\",\n",
    "    \"random_state\": 42,\n",
    "}\n",
    "\n",
    "kmeans = KMeans(**kmeans_kwargs)\n",
    "\n",
    "# Fit the algorithm to the features\n",
    "kmeans.fit(X_cluster)\n",
    "\n",
    "# Save the model results\n",
    "cluster_results[\"kmeans\"] = kmeans\n",
    "\n",
    "print(\"Time elapsed: {} seconds\".format(time.time()-time_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of iterations required to converge\n",
    "kmeans.n_iter_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final locations of the centroid\n",
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the labels\n",
    "labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The lowest SSE value\n",
    "print(\"SSE (Inertia): {}\".format(kmeans.inertia_.round(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the \"silhouette score\" for the algorithm\n",
    "print(\"Silhouette Score: {}\".format(silhouette_score(X_cluster, labels).round(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the \"calinski harabasz score\" for the algorithm\n",
    "print(\"Calinski Harabasz Score: {}\".format(calinski_harabasz_score(X_cluster, labels).round(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the \"davies bouldin score\" for the algorithm\n",
    "print(\"Davies Bouldin Score: {}\".format(davies_bouldin_score(X_cluster, labels).round(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization of Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset utilized for clustering may have used more than 2 components\n",
    "# So, take the original processed matrix \"X\" and project it onto 2 dimensions\n",
    "# Then, attach the cluster labels to the final outputted dataframe\n",
    "\n",
    "df_pca = prepare_pca(2, X, labels)\n",
    "#df_tsne = prepare_tsne(2, X, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2d(df_pca, kmeans, centers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset utilized for clustering may have more than 3 components\n",
    "# So, take the original processed matrix \"X\" and project it onto 3 dimensions\n",
    "# Then, attach the cluster labels to the final outputted dataframe\n",
    "\n",
    "df_pca = prepare_pca(3, X, labels)\n",
    "#df_tsne = prepare_tsne(3, X, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_3d(df_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-Batch KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mini-Batch K-Means is a modified version of k-means that makes updates to the cluster centroids using mini-batches of samples rather than the entire dataset, which can make it faster for large datasets, and perhaps more robust to statistical noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose the Appropriate Number of Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists that hold the values of the different internal metrics for each k\n",
    "\n",
    "time_start = time.time()\n",
    "\n",
    "silhouette = []\n",
    "calinski_harabasz = []\n",
    "davies_bouldin = []\n",
    "\n",
    "for k in range(2, max_clusters+1):\n",
    "    model = MiniBatchKMeans(n_clusters=k)\n",
    "    model.fit(X_cluster)\n",
    "    labels = model.fit_predict(X_cluster)\n",
    "    \n",
    "    score = silhouette_score(X_cluster, labels)\n",
    "    silhouette.append(score)\n",
    "    \n",
    "    score = calinski_harabasz_score(X_cluster, labels)\n",
    "    calinski_harabasz.append(score)\n",
    "    \n",
    "    score = davies_bouldin_score(X_cluster, labels)\n",
    "    davies_bouldin.append(score)\n",
    "    \n",
    "print(\"Time elapsed: {} seconds\".format(time.time()-time_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Internal Validity Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Silhouette Coefficient: maximum class spread/variance; the higher number the better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_internal_validity(max_clusters, silhouette, \"Silhouette\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calinski-Harabasz Index: interclass-intraclass distance ratio; the higher the number the better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_internal_validity(max_clusters, calinski_harabasz, \"Calinski Harabasz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Davies-Bouldin Criteria: maximum interclass-intraclass distance ratio; the lower the number the better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_internal_validity(max_clusters, davies_bouldin, \"Davies Bouldin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The Gap Statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The \"Gap Statistic\" module allows using any clustering algorithm\n",
    "# This function takes X (data) k, and func (the chosen clustering algorithm)\n",
    "# It returns a tuple of the centorid locations, and the labels assigned to X\n",
    "\n",
    "def special_clustering_func(X, k):\n",
    "    \"\"\" \n",
    "    Special clustering function which uses the MeanShift\n",
    "    model from sklearn.\n",
    "    \n",
    "    These user defined functions *must* take the X and a k \n",
    "    and can take an arbitrary number of other kwargs, which can\n",
    "    be pass with `clusterer_kwargs` when initializing OptimalK\n",
    "    \"\"\"\n",
    "    \n",
    "    # Here you can do whatever clustering algorithm you heart desires,\n",
    "    # but we'll do a simple wrap of the MeanShift model in sklearn.\n",
    "    \n",
    "    m = MiniBatchKMeans()\n",
    "    m.fit(X)\n",
    "    \n",
    "    # Return the location of each cluster center,\n",
    "    # and the labels for each point.\n",
    "    return m.cluster_centers_, m.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an \"optimalK\" object\n",
    "optimalK = OptimalK(clusterer=special_clustering_func)\n",
    "optimalK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call \"optimalK\" with a list of clusters to fit to\n",
    "n_clusters = optimalK(X_cluster, n_refs=3, cluster_array=np.arange(1, 20))\n",
    "print('Optimal clusters: ', n_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A DataFrame of gap values with each passed cluster count is now available\n",
    "#optimalK.gap_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the n_clusters against their gap values\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "plt.plot(optimalK.gap_df.n_clusters, optimalK.gap_df.gap_value, linewidth=3)\n",
    "plt.scatter(optimalK.gap_df[optimalK.gap_df.n_clusters == n_clusters].n_clusters,\n",
    "            optimalK.gap_df[optimalK.gap_df.n_clusters == n_clusters].gap_value, s=250, c='r')\n",
    "plt.grid(True)\n",
    "plt.xlabel('Cluster Count')\n",
    "plt.ylabel('Gap Value')\n",
    "plt.title('Gap Values by Cluster Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model with Optimal Number of Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the optimal number of clusters\n",
    "optimal_clusters = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "minibatch_kmeans = MiniBatchKMeans(n_clusters=optimal_clusters)\n",
    "\n",
    "# Fit the algorithm to the features\n",
    "minibatch_kmeans.fit(X_cluster)\n",
    "\n",
    "# Save the model results\n",
    "cluster_results[\"minibatch_kmeans\"] = minibatch_kmeans\n",
    "\n",
    "print(\"Time elapsed: {} seconds\".format(time.time()-time_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of iterations required to converge\n",
    "minibatch_kmeans.n_iter_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final locations of the centroid\n",
    "minibatch_kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the labels\n",
    "labels = minibatch_kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The lowest SSE value\n",
    "print(\"SSE (Inertia): {}\".format(minibatch_kmeans.inertia_.round(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the \"silhouette score\" for the algorithm\n",
    "print(\"Silhouette Score: {}\".format(silhouette_score(X_cluster, labels).round(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the \"calinski harabasz score\" for the algorithm\n",
    "print(\"Calinski Harabasz Score: {}\".format(calinski_harabasz_score(X_cluster, labels).round(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the \"davies bouldin score\" for the algorithm\n",
    "print(\"Davies Bouldin Score: {}\".format(davies_bouldin_score(X_cluster, labels).round(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization of Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset utilized for clustering may have used more than 2 components\n",
    "# So, take the original processed matrix \"X\" and project it onto 2 dimensions\n",
    "# Then, attach the cluster labels to the final outputted dataframe\n",
    "\n",
    "df_pca = prepare_pca(2, X, labels)\n",
    "#df_tsne = prepare_tsne(2, X, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2d(df_pca, minibatch_kmeans, centers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset utilized for clustering may have more than 3 components\n",
    "# So, take the original processed matrix \"X\" and project it onto 3 dimensions\n",
    "# Then, attach the cluster labels to the final outputted dataframe\n",
    "\n",
    "df_pca = prepare_pca(3, X, labels)\n",
    "#df_tsne = prepare_tsne(3, X, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_3d(df_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectral Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Characteristics\n",
    "\n",
    "*Parameters:* number of clusters\n",
    "\n",
    "*Scalability:* Medium n_samples, small n_clusters\n",
    "\n",
    "*Usecase:* Few clusters, even clsuter size, non-flat geometry\n",
    "\n",
    "*Geometry (metric used):* Graph distance (e.g., nearest-neighbor graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose the Appropriate Number of Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "# Create lists that hold the values of the different internal metrics for each k\n",
    "silhouette = []\n",
    "calinski_harabasz = []\n",
    "davies_bouldin = []\n",
    "\n",
    "for k in range(2, max_clusters+1):\n",
    "    model = SpectralClustering(n_clusters=k, affinity='nearest_neighbors', n_neighbors=15)\n",
    "    model.fit(X_cluster)\n",
    "    labels = model.fit_predict(X_cluster)\n",
    "    \n",
    "    score = silhouette_score(X_cluster, labels)\n",
    "    silhouette.append(score)\n",
    "    \n",
    "    score = calinski_harabasz_score(X_cluster, labels)\n",
    "    calinski_harabasz.append(score)\n",
    "    \n",
    "    score = davies_bouldin_score(X_cluster, labels)\n",
    "    davies_bouldin.append(score)\n",
    "    \n",
    "print(\"Time elapsed: {} seconds\".format(time.time()-time_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Internal Validity Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Silhouette Coefficient: maximum class spread/variance; the higher number the better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_internal_validity(max_clusters, silhouette, \"Silhouette\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calinski-Harabasz Index: interclass-intraclass distance ratio; the higher the number the better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_internal_validity(max_clusters, calinski_harabasz, \"Calinski Harabasz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Davies-Bouldin Criteria: maximum interclass-intraclass distance ratio; the lower the number the better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_internal_validity(max_clusters, davies_bouldin, \"David Bouldin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model with Optimal Number of Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the optimal number of clusters\n",
    "optimal_clusters = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "spectral_clustering = SpectralClustering(n_clusters=optimal_clusters, affinity='nearest_neighbors', n_neighbors=15)\n",
    "\n",
    "# Fit the algorithm to the features\n",
    "spectral_clustering.fit(X_cluster)\n",
    "\n",
    "# Save the model results\n",
    "cluster_results[\"spectral_clustering\"] = spectral_clustering\n",
    "\n",
    "print(\"Time elapsed: {} seconds\".format(time.time()-time_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the labels\n",
    "labels = spectral_clustering.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the \"silhouette score\" for the algorithm\n",
    "print(\"Silhouette Score: {}\".format(silhouette_score(X_cluster, labels).round(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the \"calinski harabasz score\" for the algorithm\n",
    "print(\"Calinski Harabasz Score: {}\".format(calinski_harabasz_score(X_cluster, labels).round(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the \"davies bouldin score\" for the algorithm\n",
    "print(\"Davies Bouldin Score: {}\".format(davies_bouldin_score(X_cluster, labels).round(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization of Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset utilized for clustering may have used more than 2 components\n",
    "# So, take the original processed matrix \"X\" and project it onto 2 dimensions\n",
    "# Then, attach the cluster labels to the final outputted dataframe\n",
    "\n",
    "df_pca = prepare_pca(2, X, labels)\n",
    "#df_tsne = prepare_tsne(2, X, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2d(df_pca, spectral_clustering, centers=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset utilized for clustering may have more than 3 components\n",
    "# So, take the original processed matrix \"X\" and project it onto 3 dimensions\n",
    "# Then, attach the cluster labels to the final outputted dataframe\n",
    "\n",
    "df_pca = prepare_pca(3, X, labels)\n",
    "#df_tsne = prepare_tsne(3, X, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_3d(df_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the appropriate Dimensionality Reduction technique and number of components \n",
    "\n",
    "# scikit-learn chooses the minimum number of principal components such that 95% of the variance is retained.\n",
    "pca = PCA(.95, svd_solver=\"full\") \n",
    "\n",
    "# Select the number of components for PCA\n",
    "#pca = PCA(n_components=2, svd_solver='full')\n",
    "\n",
    "# Transform the data\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Select X_pca or X_pca_tsne\n",
    "X_cluster = X_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the maximum number of clusters to test\n",
    "max_clusters = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ward Linkage (Agglomerative Clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Characteristics\n",
    "\n",
    "*Parameters:* number of clusters or distance threshold\n",
    "\n",
    "*Scalability:* Large n_samples and n_clusters\n",
    "\n",
    "*Usecase:* Many clusters, possibly connectivity constraints\n",
    "\n",
    "*Geometry (metric used):* distances between points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose the Appropriate Number of Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "# Create lists that hold the values of the different internal metrics for each k\n",
    "silhouette = []\n",
    "calinski_harabasz = []\n",
    "davies_bouldin = []\n",
    "\n",
    "for k in range(2, max_clusters+1):\n",
    "    model = AgglomerativeClustering(linkage=\"ward\", n_clusters=k)\n",
    "    model.fit(X_cluster)\n",
    "    labels = model.fit_predict(X_cluster)\n",
    "    \n",
    "    score = silhouette_score(X_cluster, labels)\n",
    "    silhouette.append(score)\n",
    "    \n",
    "    score = calinski_harabasz_score(X_cluster, labels)\n",
    "    calinski_harabasz.append(score)\n",
    "    \n",
    "    score = davies_bouldin_score(X_cluster, labels)\n",
    "    davies_bouldin.append(score)\n",
    "    \n",
    "print(\"Time elapsed: {} seconds\".format(time.time()-time_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Internal Validity Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Silhouette Coefficient: maximum class spread/variance; the higher number the better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_internal_validity(max_clusters, silhouette, \"Silhouette\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calinski-Harabasz Index: interclass-intraclass distance ratio; the higher the number the better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_internal_validity(max_clusters, calinski_harabasz, \"Calinski Harabasz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Davies-Bouldin Criteria: maximum interclass-intraclass distance ratio; the lower the number the better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_internal_validity(max_clusters, davies_bouldin, \"Davies Bouldin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model with Optimal Number of Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the optimal number of clusters\n",
    "optimal_clusters = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "hierarchical_ward = AgglomerativeClustering(linkage=\"ward\", n_clusters=optimal_clusters)\n",
    "\n",
    "# Fit the algorithm to the features\n",
    "hierarchical_ward.fit(X_cluster)\n",
    "\n",
    "# Save the model results\n",
    "cluster_results[\"hierarchical_ward\"] = hierarchical_ward\n",
    "\n",
    "print(\"Time elapsed: {} seconds\".format(time.time()-time_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the labels\n",
    "labels = hierarchical_ward.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the \"silhouette score\" for the algorithm\n",
    "print(\"Silhouette Score: {}\".format(silhouette_score(X_cluster, labels).round(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the \"calinski harabasz score\" for the algorithm\n",
    "print(\"Calinski Harabasz Score: {}\".format(calinski_harabasz_score(X_cluster, labels).round(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the \"davies bouldin score\" for the algorithm\n",
    "print(\"Davies Bouldin Score: {}\".format(davies_bouldin_score(X_cluster, labels).round(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization of Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset utilized for clustering may have used more than 2 components\n",
    "# So, take the original processed matrix \"X\" and project it onto 2 dimensions\n",
    "# Then, attach the cluster labels to the final outputted dataframe\n",
    "\n",
    "df_pca = prepare_pca(2, X, labels)\n",
    "#df_tsne = prepare_tsne(2, X, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2d(df_pca, hierarchical_ward, centers=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset utilized for clustering may have more than 3 components\n",
    "# So, take the original processed matrix \"X\" and project it onto 3 dimensions\n",
    "# Then, attach the cluster labels to the final outputted dataframe\n",
    "\n",
    "df_pca = prepare_pca(3, X, labels)\n",
    "#df_tsne = prepare_tsne(3, X, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_3d(df_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the appropriate Dimensionality Reduction technique and number of components \n",
    "\n",
    "# scikit-learn chooses the minimum number of principal components such that 95% of the variance is retained.\n",
    "#pca = PCA(.95, svd_solver=\"full\") \n",
    "\n",
    "# Select the number of components for PCA\n",
    "pca = PCA(n_components=2, svd_solver='full')\n",
    "\n",
    "# Transform the data\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Select X_pca or X_pca_tsne\n",
    "X_cluster = X_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the maximum number of clusters to test\n",
    "max_clusters = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BIRCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Characteristics\n",
    "\n",
    "*Parameters:* branching factor, treshold, optional global clusterer\n",
    "\n",
    "*Scalability:* Large n_clusters and n_samples. BIRCH does not scale very well to high dimensional data. As a rule of thumb if n_features is greater than twenty, it is generally better to use MiniBatchKMeans.\n",
    "\n",
    "*Usecase:* Large dataset, outlier removal, data reduction\n",
    "\n",
    "*Geometry (metric used):* Euclidean distance between points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose the Appropriate Number of Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "# Create lists that hold the values of the different internal metrics for each k\n",
    "silhouette = []\n",
    "calinski_harabasz = []\n",
    "davies_bouldin = []\n",
    "\n",
    "for k in range(2, max_clusters+1):\n",
    "    model = Birch(threshold=0.01, n_clusters=k)\n",
    "    model.fit(X_cluster)\n",
    "    labels = model.fit_predict(X_cluster)\n",
    "    \n",
    "    score = silhouette_score(X_cluster, labels)\n",
    "    silhouette.append(score)\n",
    "    \n",
    "    score = calinski_harabasz_score(X_cluster, labels)\n",
    "    calinski_harabasz.append(score)\n",
    "    \n",
    "    score = davies_bouldin_score(X_cluster, labels)\n",
    "    davies_bouldin.append(score)\n",
    "    \n",
    "print(\"Time elapsed: {} seconds\".format(time.time()-time_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Internal Validity Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Silhouette Coefficient: maximum class spread/variance; the higher number the better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_internal_validity(max_clusters, silhouette, \"Silhouette\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calinski-Harabasz Index: interclass-intraclass distance ratio; the higher the number the better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_internal_validity(max_clusters, calinski_harabasz, \"Calinski Harabasz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Davies-Bouldin Criteria: maximum interclass-intraclass distance ratio; the lower the number the better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_internal_validity(max_clusters, davies_bouldin, \"Davies Bouldin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model with Optimal Number of Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the optimal number of clusters\n",
    "optimal_clusters = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "hierarchical_birch = Birch(threshold=0.01, n_clusters=optimal_clusters)\n",
    "\n",
    "# Fit the algorithm to the features\n",
    "hierarchical_birch.fit(X_cluster)\n",
    "\n",
    "# Save the model results\n",
    "cluster_results[\"hierarchical_birch\"] = hierarchical_birch\n",
    "\n",
    "print(\"Time elapsed: {} seconds\".format(time.time()-time_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the labels\n",
    "labels = hierarchical_birch.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the \"silhouette score\" for the algorithm\n",
    "print(\"Silhouette Score: {}\".format(silhouette_score(X_cluster, labels).round(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the \"calinski harabasz score\" for the algorithm\n",
    "print(\"Calinski Harabasz Score: {}\".format(calinski_harabasz_score(X_cluster, labels).round(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the \"davies bouldin score\" for the algorithm\n",
    "print(\"Davies Bouldin Score: {}\".format(davies_bouldin_score(X_cluster, labels).round(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization of Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset utilized for clustering may have used more than 2 components\n",
    "# So, take the original processed matrix \"X\" and project it onto 2 dimensions\n",
    "# Then, attach the cluster labels to the final outputted dataframe\n",
    "\n",
    "df_pca = prepare_pca(2, X, labels)\n",
    "#df_tsne = prepare_tsne(2, X, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2d(df_pca, hierarchical_birch, centers=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset utilized for clustering may have more than 3 components\n",
    "# So, take the original processed matrix \"X\" and project it onto 3 dimensions\n",
    "# Then, attach the cluster labels to the final outputted dataframe\n",
    "\n",
    "df_pca = prepare_pca(3, X, labels)\n",
    "#df_tsne = prepare_tsne(3, X, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_3d(df_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consensus Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the appropriate Dimensionality Reduction technique and number of components \n",
    "\n",
    "# scikit-learn chooses the minimum number of principal components such that 95% of the variance is retained.\n",
    "#pca = PCA(.95, svd_solver=\"full\") \n",
    "\n",
    "# Select the number of components for PCA\n",
    "pca = PCA(n_components=2, svd_solver='full')\n",
    "\n",
    "# Transform the data\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Select X_pca or X_pca_tsne\n",
    "X_cluster = X_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the maximum number of clusters to test\n",
    "max_clusters = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Consensus Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_clusters = Number of clusters\n",
    "\n",
    "n_clusters_base = Number of clusters to use the base classifier\n",
    "\n",
    "n_components = Number of components of the consensus\n",
    "\n",
    "ncb_rand = If the number of clusters of each component is chosen randomly in the interval [ 2..n_clusters ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 2\n",
    "km = KMeans(n_clusters=k)\n",
    "\n",
    "cons = SimpleConsensusClustering(n_clusters=k, n_clusters_base=4, n_components=10, ncb_rand=False)\n",
    "\n",
    "lkm = km.fit_predict(X_cluster)\n",
    "cons.fit(X_cluster)\n",
    "lcons = cons.labels_\n",
    "\n",
    "print('K-M SS =', silhouette_score(X_cluster, labels))\n",
    "print('SCC SS =', silhouette_score(X_cluster, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the Constructed Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle the \"cluster_results\" dictionary\n",
    "with open(\"cluster_results.obj\", 'wb') as fp:\n",
    "    pickle.dump(cluster_results, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Internal Validity Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the appropriate Dimensionality Reduction technique and number of components \n",
    "\n",
    "# scikit-learn chooses the minimum number of principal components such that 95% of the variance is retained.\n",
    "#pca = PCA(.95, svd_solver=\"full\") \n",
    "\n",
    "# Select the number of components for PCA\n",
    "pca = PCA(n_components=2, svd_solver='full')\n",
    "\n",
    "# Transform the data\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Select X_pca or X_pca_tsne\n",
    "X_cluster = X_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = []\n",
    "silhouette = []\n",
    "calinski_harabasz = []\n",
    "davies_bouldin = []\n",
    "\n",
    "for key in cluster_results.keys():\n",
    "    model_names.append(key)\n",
    "    model = cluster_results[key]\n",
    "    labels = model.labels_\n",
    "    \n",
    "    score = silhouette_score(X_cluster, labels)\n",
    "    silhouette.append(score)\n",
    "    \n",
    "    score = calinski_harabasz_score(X_cluster, labels)\n",
    "    calinski_harabasz.append(score)\n",
    "    \n",
    "    score = davies_bouldin_score(X_cluster, labels)\n",
    "    davies_bouldin.append(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silhouette Coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Silhouette Coefficient: maximum class spread/variance; the higher number the better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "ax.set_ylabel(\"Silhouette\")\n",
    "ax.set_xlabel(\"Clustering Method\")\n",
    "ax.set_title(\"Model Results: Silhouette Coefficient\")\n",
    "ax.bar(model_names, silhouette)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calinski-Harabasz Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calinski-Harabasz Index: interclass-intraclass distance ratio; the higher the number the better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "ax.set_ylabel(\"Calinski-Harabasz\")\n",
    "ax.set_xlabel(\"Clustering Method\")\n",
    "ax.set_title(\"Model Results: Calinski-Harabasz Index\")\n",
    "ax.bar(model_names, calinski_harabasz)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Davies-Bouldin Criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Davies-Bouldin Criteria: maximum interclass-intraclass distance ratio; the lower the number the better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "ax.set_ylabel(\"Davies-Bouldin\")\n",
    "ax.set_xlabel(\"Clustering Method\")\n",
    "ax.set_title(\"Model Results: Davies-Bouldin Criteria\")\n",
    "ax.bar(model_names, davies_bouldin)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the Final Chosen Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the model that produces the best internal validity measures\n",
    "chosen_model = cluster_results[\"kmeans\"]\n",
    "labels = chosen_model.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization of Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset utilized for clustering may have used more than 2 components\n",
    "# So, take the original processed matrix \"X\" and project it onto 2 dimensions\n",
    "# Then, attach the cluster labels to the final outputted dataframe\n",
    "\n",
    "df_pca = prepare_pca(2, X, labels)\n",
    "#df_tsne = prepare_tsne(2, X, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2d(df_pca, chosen_model, centers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset utilized for clustering may have more than 3 components\n",
    "# So, take the original processed matrix \"X\" and project it onto 3 dimensions\n",
    "# Then, attach the cluster labels to the final outputted dataframe\n",
    "\n",
    "df_pca = prepare_pca(3, X, labels)\n",
    "#df_tsne = prepare_tsne(3, X, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_3d(df_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the clusters have been created, it would be nice to determine what makes each one unique. This will help with the understanding of the different observations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance Within Variables and Between Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go back the original dataframe with the raw data that has all the features (i.e., df_viz)\n",
    "df_temp = df_viz[df_viz.columns[~df_viz.columns.isin([\"GEOID\",\"GEO_ID\",\"NAME\"])]].copy()\n",
    "\n",
    "# Filter out the rows that weren't used for clustering\n",
    "#temp = list(set(list(df[\"GEOID\"])).difference(set(list(df_temp[\"GEOID\"]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting all variables between 0 and 1 in order to better visualize the results\n",
    "scaler = MinMaxScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df_temp))\n",
    "df_scaled.columns = df_temp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the labels of the chosen model\n",
    "df_temp[\"labels\"] = chosen_model.labels_\n",
    "df_scaled[\"labels\"] = chosen_model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate variables with largest differences (by standard deviation)\n",
    "# The higher the standard deviation in a variable based on average values for each cluster\n",
    "# The more likely that the variable is important when creating the cluster\n",
    "df_mean = df_scaled.loc[df_scaled.labels!=-1, :].groupby('labels').mean().reset_index()\n",
    "\n",
    "results = pd.DataFrame(columns=['Variable', 'Std'])\n",
    "\n",
    "for column in df_mean.columns[1:]:\n",
    "    results.loc[len(results), :] = [column, np.std(df_mean[column])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the number of columns to evaluate\n",
    "num_cols = 5\n",
    "\n",
    "# Put the contents of the top columns in a list\n",
    "selected_columns = list(results.sort_values('Std', ascending=False)\n",
    "                        .head(num_cols).Variable.values) + [\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot data\n",
    "tidy = df_scaled[selected_columns].melt(id_vars='labels')\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "sns.barplot(x='labels', y='value', hue='variable', data=tidy, palette='Set3')\n",
    "plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heatmap of Important Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the scaled dataframe for the labels and important featues\n",
    "df_scaled_sub = df_scaled[selected_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataframe into cluster groups.\n",
    "# Then, compute the mean for all columns in every group\n",
    "df_grouped = df_scaled_sub.groupby([\"labels\"], sort=True).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the column labels in a list\n",
    "labels = list(df_scaled_sub.labels.unique())\n",
    "labels.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [go.Heatmap(z=df_grouped.values.tolist(), \n",
    "                   y=labels,\n",
    "                   x=list(df_grouped.columns),\n",
    "                   colorscale='Viridis')]\n",
    "\n",
    "plotly.offline.iplot(data, filename='pandas-heatmap')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Density Plots of Important Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(selected_columns[:-1])):\n",
    "\n",
    "    df_density = pd.DataFrame(df_scaled_sub[df_scaled_sub[\"labels\"]==0].iloc[:,i])\n",
    "    col = list(df_density.columns)[0]\n",
    "    df_density.rename(columns={col:\"0\"}, inplace=True)\n",
    "\n",
    "    for l in labels[1:]:\n",
    "        new_col = df_scaled_sub[df_scaled_sub[\"labels\"]==l].iloc[:,i]\n",
    "        df_density = pd.concat([df_density, new_col], axis=1, sort=False)\n",
    "        df_density.rename(columns={col:str(l)}, inplace=True)\n",
    "    \n",
    "    print(\"Density Plot for Feature: \" + selected_columns[i])\n",
    "    df_density.plot.kde()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
