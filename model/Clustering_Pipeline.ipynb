{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Data Handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import scale, normalize, StandardScaler, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Dimensionality Reduction\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Clustering\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "\n",
    "# Evaluation\n",
    "from pyclustertend import hopkins, vat, ivat\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "from kneed import KneeLocator\n",
    "import gapstat_rs\n",
    "from gap_statistic import OptimalK\n",
    "from sklearn.metrics import silhouette_score\n",
    "from amltlearn.metrics.cluster import calinski_harabasz_score, davies_bouldin_score\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import mpl_toolkits.mplot3d.axes3d as p3\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import animation\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_corr(df):\n",
    "    corr = df.corr()\n",
    "    mask = np.zeros_like(corr, dtype=np.bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "    f, ax = plt.subplots(figsize=(11, 9))\n",
    "    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "                square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The \"Gap Statistic\" module allows using any clustering algorithm\n",
    "# This function takes X (data) k, and func (the chosen clustering algorithm)\n",
    "# It returns a tuple of the centorid locations, and the labels assigned to X\n",
    "\n",
    "def special_clustering_func(X, k, func):\n",
    "    \"\"\" \n",
    "    Special clustering function which uses the MeanShift\n",
    "    model from sklearn.\n",
    "    \n",
    "    These user defined functions *must* take the X and a k \n",
    "    and can take an arbitrary number of other kwargs, which can\n",
    "    be pass with `clusterer_kwargs` when initializing OptimalK\n",
    "    \"\"\"\n",
    "    \n",
    "    # Here you can do whatever clustering algorithm you heart desires,\n",
    "    # but we'll do a simple wrap of the MeanShift model in sklearn.\n",
    "    \n",
    "    m = func\n",
    "    m.fit(X)\n",
    "    \n",
    "    # Return the location of each cluster center,\n",
    "    # and the labels for each point.\n",
    "    return m.cluster_centers_, m.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_pca(n_components, data, labels):\n",
    "    matrix = PCA(n_components=n_components).fit_transform(data)\n",
    "    \n",
    "    names = ['x', 'y', 'z']\n",
    "    df_matrix = pd.DataFrame(matrix)\n",
    "    df_matrix.rename({i:names[i] for i in range(n_components)}, axis=1, inplace=True)\n",
    "    df_matrix['labels'] = labels\n",
    "    \n",
    "    return df_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_tsne(n_components, data, labels):\n",
    "    pca = PCA(.95) \n",
    "    X_pca = pca_n.fit_transform(data)\n",
    "    \n",
    "    tsne = TSNE(n_components=n_components, verbose=0, perplexity=40, n_iter=300)\n",
    "    matrix = tsne.fit_transform(X_pca)\n",
    "    \n",
    "    names = ['x', 'y', 'z']\n",
    "    df_matrix = pd.DataFrame(matrix)\n",
    "    df_matrix.rename({i:names[i] for i in range(n_components)}, axis=1, inplace=True)\n",
    "    df_matrix['labels'] = labels\n",
    "    \n",
    "    return df_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(r'2018_5yr_cendatagov_ESTIMATES_v3.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(axis=1, how='any', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Sources\n",
    "\n",
    "###### https://scikit-learn.org/stable/modules/preprocessing.html\n",
    "\n",
    "###### https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html#sphx-glr-auto-examples-preprocessing-plot-scaling-importance-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Scaling features is a requirement for the optimal performance of many machine learning algorithms.\n",
    "\n",
    "###### Use StandardScaler to help standardize the dataset’s features onto unit scale (mean = 0 and variance = 1).\n",
    "\n",
    "###### Use the MinMaxScaler for feature scaling when we do not assume that the shape of all the features follows a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather all the values from the feature columns of the dataframe\n",
    "X_raw = df[df.columns[~df.columns.isin([\"GEOID\",\"GEO_ID\",\"NAME\"])]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use StandardScaler to help standardize the dataset’s features onto unit scale (mean = 0 and variance = 1)\n",
    "X = StandardScaler().fit_transform(X_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(n=10000, axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_raw = df[df.columns[~df.columns.isin([\"GEOID\",\"GEO_ID\",\"NAME\"])]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = StandardScaler().fit_transform(X_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explorartory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Sources\n",
    "\n",
    "###### https://www.geeksforgeeks.org/multidimensional-data-analysis-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating correlation heatmap \n",
    "sns.heatmap(df[df.columns[~df.columns.isin([\"GEOID\",\"GEO_ID\",\"NAME\"])]].corr(), annot = True) \n",
    "  \n",
    "# posting correlation heatmap to output console  \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_corr(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trisurface Plot for Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating correlation data \n",
    "df = df[df.columns[~df.columns.isin([\"GEOID\",\"GEO_ID\",\"NAME\"])]].corr() \n",
    "df.index = range(0, len(df)) \n",
    "df.rename(columns = dict(zip(df.columns, df.index)), inplace = True) \n",
    "df = df.astype(object) \n",
    "  \n",
    "# Generating coordinates with corresponding correlation values \n",
    "for i in range(0, len(df)): \n",
    "    for j in range(0, len(df)): \n",
    "        if i != j: \n",
    "            df.iloc[i, j] = (i, j, df.iloc[i, j]) \n",
    "        else: \n",
    "            df.iloc[i, j] = (i, j, 0) \n",
    "\n",
    "df_list = [] \n",
    "  \n",
    "# flattening dataframe values \n",
    "for sub_list in df.values: \n",
    "    df_list.extend(sub_list) \n",
    "\n",
    "# converting list of tuples into trivariate dataframe \n",
    "plot_df = pd.DataFrame(df_list) \n",
    "  \n",
    "fig = plt.figure() \n",
    "ax = Axes3D(fig) \n",
    "  \n",
    "# plotting 3D trisurface plot \n",
    "ax.plot_trisurf(plot_df[0], plot_df[1], plot_df[2],  \n",
    "                    cmap = cm.jet, linewidth = 0.2) \n",
    "  \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Tendency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Sources\n",
    "\n",
    "###### https://pypi.org/project/pyclustertend/\n",
    "###### https://pyclustertend.readthedocs.io/en/latest/\n",
    "###### https://www.kaggle.com/lachhebo/hopkins-test\n",
    "###### https://medium.com/@stevenzook_98922/regarding-the-hopkins-test-beware-of-the-implementation-you-use-as-the-value-returned-may-be-2f6db7849987\n",
    "###### https://www.datanovia.com/en/lessons/assessing-clustering-tendency/\n",
    "###### https://stats.stackexchange.com/questions/332651/validating-cluster-tendency-using-hopkins-statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Before applying any clustering method on the data, it’s important to evaluate whether the data sets contains meaningful clusters (i.e.: non-random structures) or not. This process is defined as the assessing of clustering tendency or the feasibility of the clustering analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "X_scale = scale(X_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hopkins Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The Hopkins statistic is used to assess the clustering tendency of a data set by measuring the probability that a given data set is generated by a uniform data distribution. In other words, it tests the spatial randomness of the data.\n",
    "\n",
    "###### Three different results are possible: 1) H = 0.5: the dataset reveals no clustering structure in the formula; 2) H close to 0, a significant evidence that the data might be cluster-able; 3) H is close to 1, in this case the test is indecisive (data are neither clustered nor random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hopkins(X_scale, X_scale.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### VAT (visual assessment of tendency) is an algorithm which creates a visualisation of a specific dataset, which can be useful to obtain an insight on the number of clusters and cluster hierarchy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vat(X_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ivat algorithm is a improved version of the vat algorithm which produce more precise images at the cost of a heavier computing cost\n",
    "#ivat(X_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Sources\n",
    "###### https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60\n",
    "###### https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html#Choosing-the-number-of-components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### PCA works by using orthogonal transformations to convert correlates features into a set of values of linearly uncorrelated features. What is left are features that contain the largest possible variance. One of the most important applications of PCA is for speeding up machine learning algorithms. \n",
    "\n",
    "###### Although PCA might be successful in reducing the dimensionality of the data, it does not seem to visualize clusters very intuitively. This happens often with high dimensional data because it is typically clustered around the same point and PCA extracts that information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the Coordinates of the Right-Singular Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape\n",
    "print(\"X:\", X.shape)\n",
    "\n",
    "# Find the minimum dimension\n",
    "s = min(X.shape)\n",
    "print(\"s = min({}, {}) == {}\".format(X.shape[0], X.shape[1], s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Singular Value Decomposition\n",
    "U, Sigma, VT = np.linalg.svd(X, full_matrices=False)\n",
    "\n",
    "print(\"U:\", U.shape)\n",
    "print(\"Sigma:\", Sigma.shape)\n",
    "print(\"VT:\", VT.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the coordinates of the top two (k_approx, below) right-singular vectors\n",
    "\n",
    "m, d = X.shape\n",
    "k_approx = 2\n",
    "assert k_approx <= s\n",
    "\n",
    "# Plot the components of the first k_approx=2 singular vectors\n",
    "fig, axs = plt.subplots(1, k_approx, sharex=True, sharey=True,\n",
    "                        figsize=(10*k_approx, 10))\n",
    "for k in range(k_approx):\n",
    "    axs[k].scatter(np.arange(min(m, d)), VT[k, :].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D Scatter Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA with sklearn\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "print('Explained variation per principal component: {} \\n'\n",
    "      .format(pca.explained_variance_ratio_))\n",
    "\n",
    "print('Cumulative explained variation of the principal components: {}'.format(np.sum(pca.explained_variance_ratio_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's possible to take the original data and project it onto the 2-dimensional subspace defined by the first two right-singular vectors.\n",
    "df_pca = pd.DataFrame(data=X_pca, columns=['component_1', 'component_2'])\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "plt.scatter(df_pca[\"component_1\"], df_pca[\"component_2\"])\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.axis('square')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-SNE\n",
    "### (with Prior Dimensionality Reduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Sources\n",
    "###### https://towardsdatascience.com/visualising-high-dimensional-datasets-using-pca-and-t-sne-in-python-8ef87e7915b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### t-Distributed Stochastic Neighbor Embedding (t-SNE) is another technique for dimensionality reduction and is particularly well suited for the visualization of high-dimensional datasets. Contrary to PCA it is not a mathematical technique but a probablistic one.\n",
    "\n",
    "###### t-SNE minimizes the divergence between two distributions: a distribution that measures pairwise similarities of the input objects and a distribution that measures pairwise similarities of the corresponding low-dimensional points in the embedding\n",
    "\n",
    "###### Since t-SNE scales quadratically in the number of objects N, its applicability is limited to data sets with only a few thousand input objects; beyond that, learning becomes too slow to be practical (and the memory requirements become too large)\n",
    "\n",
    "###### It is highly recommended to use another dimensionality reduction method (e.g. PCA for dense data or TruncatedSVD for sparse data) before using t-SNE to reduce the number of dimensions to a reasonable amount (e.g. 50) if the number of features is very high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scikit-learn chooses the minimum number of principal components such that 95% of the variance is retained.\n",
    "#pca = PCA(.95) \n",
    "\n",
    "# Select the number of components for PCA\n",
    "pca_n = PCA(n_components=50)\n",
    "X_pca_n = pca_n.fit_transform(X)\n",
    "\n",
    "# The explained variance tells you how much information (variance) can be attributed to each of the principal components.\n",
    "print('Cumulative explained variation for the principal components: {}'.format(np.sum(pca_n.explained_variance_ratio_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "tsne = TSNE(n_components=2, verbose=0, perplexity=40, n_iter=300)\n",
    "X_pca_tsne = tsne.fit_transform(X_pca_n)\n",
    "\n",
    "print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))\n",
    "\n",
    "df_pca_tsne = pd.DataFrame(data=X_pca_tsne, columns=['component_1', 'component_2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D Scatter Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "sns.scatterplot(\n",
    "    x=\"component_1\", y=\"component_2\",\n",
    "    palette=sns.color_palette(\"hls\", 10),\n",
    "    data=df_pca_tsne,\n",
    "    legend=\"full\",\n",
    "    alpha=0.3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "plt.scatter(df_pca_tsne[\"component_1\"], df_pca_tsne[\"component_2\"])\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.axis('square')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose the Appropriate Number of Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### By setting n_components=2, we are compacting all of the features into two dimensions. This value is convenient for visualization on a two-dimensional plot.\n",
    "\n",
    "###### However, only using two components means that the not all of the explained variance of the input data will be captures. Explained variance measures the discrepancy between the transformed data and the actual input data.\n",
    "\n",
    "###### For the model, it will be important to conduct parameter tuning because it is a powerful method to maximize performance from clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "kmeans_kwargs = {\n",
    "    \"init\": \"random\",\n",
    "    \"n_clusters\": n_clusters,\n",
    "    \"n_init\": 10,\n",
    "    \"max_iter\": 300,\n",
    "    \"random_state\": 42,\n",
    "}\n",
    "\n",
    "# Empty list to hold evaluation metrics\n",
    "silhouette_scores = []\n",
    "for n in range(2, 11):\n",
    "    # This set the number of components for pca,\n",
    "    # but leaves other steps unchanged\n",
    "    pca_n = PCA(n_components=n)\n",
    "    X_pca_n = pca_n.fit_transform(X)\n",
    "\n",
    "    kmeans = KMeans(**kmeans_kwargs)\n",
    "    kmeans.fit(X_pca_n)\n",
    "    \n",
    "    silhouette_coef = silhouette_score(\n",
    "        X_pca_n,\n",
    "        kmeans.labels_,\n",
    "    )\n",
    "\n",
    "    # Add metric to the appropriate list\n",
    "    silhouette_scores.append(silhouette_coef)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(\n",
    "    range(2, 11),\n",
    "    silhouette_scores,\n",
    "    c=\"#008fd5\")\n",
    "\n",
    "plt.xlabel(\"n_components\")\n",
    "plt.title(\"Clustering Performance as a Function of n_components\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the optimal number of components\n",
    "#components = 2\n",
    "\n",
    "# Rerun PCA or t-SNE with the optimal number of components selected\n",
    "#pca = PCA(n_components=components)\n",
    "#X_cluster = pca.fit_transform(X)\n",
    "\n",
    "X_cluster = X_pca_tsne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Sources\n",
    "\n",
    "###### https://blog.floydhub.com/introduction-to-k-means-clustering-in-python-with-scikit-learn/\n",
    "###### https://realpython.com/k-means-clustering-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose the Appropriate Number of Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Elbow Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_kwargs = {\n",
    "    \"init\": \"random\",\n",
    "    \"n_init\": 10,\n",
    "    \"max_iter\": 300,\n",
    "    \"random_state\": 42,\n",
    "}\n",
    "\n",
    "# A list holds the SSE values for each k\n",
    "sse = []\n",
    "for k in range(2, 20):\n",
    "    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n",
    "    kmeans.fit(X_cluster)\n",
    "    sse.append(kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"fivethirtyeight\")\n",
    "plt.plot(range(2, 20), sse)\n",
    "plt.xticks(range(2, 20))\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"SSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl = KneeLocator(\n",
    "    range(2, 20), sse, curve=\"convex\", direction=\"decreasing\"\n",
    ")\n",
    "\n",
    "n_clusters = kl.elbow\n",
    "n_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Silhouette Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The silhouette coefficient is a measure of cluster cohesion and separation (i.e., maximum class spread/variance). It quantifies how well a data point fits into its assigned cluster based on two factors: How close the data point is to other points in the cluster; How far away the data point is from points in other clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list holds the silhouette coefficients for each k\n",
    "silhouette_coefficients = []\n",
    "\n",
    "# Notice you start at 2 clusters for silhouette coefficient\n",
    "for k in range(2, 20):\n",
    "    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n",
    "    kmeans.fit(X_cluster)\n",
    "    score = silhouette_score(X_cluster, kmeans.labels_)\n",
    "    silhouette_coefficients.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"fivethirtyeight\")\n",
    "plt.plot(range(2, 20), silhouette_coefficients)\n",
    "plt.xticks(range(2, 20))\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"Silhouette Coefficient\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calinski-Harabasz Score and Davies-Bouldin Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Calinski-Harabasz Index: interclass-intraclass distance ratio; the higher the number the better\n",
    "\n",
    "###### Davies-Bouldin Criteria: maximum interclass-intraclass distance ratio; the lower the number the better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lscores = []\n",
    "nclusters = 20\n",
    "for nc in range(2,nclusters+1):\n",
    "    km = KMeans(n_clusters=nc, n_init=10, random_state=0)\n",
    "    labels = km.fit_predict(X_cluster)\n",
    "    lscores.append((\n",
    "        calinski_harabasz_score(X_cluster, labels),\n",
    "        davies_bouldin_score(X_cluster, labels)))\n",
    "\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "ax = fig.add_subplot(131)\n",
    "plt.plot(range(2,nclusters+1), [x for x,_ in lscores])\n",
    "ax = fig.add_subplot(132)\n",
    "plt.plot(range(2,nclusters+1), [x for _, x in lscores])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Gap Statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Sources\n",
    "\n",
    "###### https://towardsdatascience.com/clustering-evaluation-strategies-98a4006fcfc\n",
    "###### https://github.com/milesgranger/gap_statistic\n",
    "###### https://github.com/milesgranger/gap_statistic/blob/master/Example.ipynb\n",
    "###### https://anaconda.org/milesgranger/gap-statistic/notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### A powerful statistical method to find the optimal number of clusters. Assess the number of clusters comparing a clustering with the expected distribution of data given the null hypothesis (no clusters) \n",
    "\n",
    "###### Computes the different clusterings of the data increasing the number of clusters and compares to clusters of data generated from a uniform distribution \n",
    "\n",
    "###### The inter-class distance matrix Sw is computed for both and compared. The correct number of clusters is where the widest gap appears between the Sw of the data and the unform data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an \"optimalK\" object\n",
    "optimalK = OptimalK(parallel_backend='rust')\n",
    "optimalK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call \"optimalK\" with a list of clusters to fit to\n",
    "n_clusters = optimalK(X_cluster, cluster_array=np.arange(1, 20))\n",
    "print('Optimal clusters: ', n_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A DataFrame of gap values with each passed cluster count is now available\n",
    "optimalK.gap_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the n_clusters against their gap values\n",
    "plt.plot(optimalK.gap_df.n_clusters, optimalK.gap_df.gap_value, linewidth=3)\n",
    "plt.scatter(optimalK.gap_df[optimalK.gap_df.n_clusters == n_clusters].n_clusters,\n",
    "            optimalK.gap_df[optimalK.gap_df.n_clusters == n_clusters].gap_value, s=250, c='r')\n",
    "plt.grid(True)\n",
    "plt.xlabel('Cluster Count')\n",
    "plt.ylabel('Gap Value')\n",
    "plt.title('Gap Values by Cluster Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The “right” number of clusters in a data set can also be determined by cross-validation. \n",
    "\n",
    "###### First, divide the given data set into m parts. Next, use m-1 parts to build a clustering model, and use the remaining part to test the quality of the clustering. For example, for each point in the test set, we can find the closest centroid. \n",
    "\n",
    "###### Consequently, we can use the sum of squared distances between all points in the test set and the closest centroids to measure how well the clustering model fits the test set. \n",
    "\n",
    "###### For any integer k > 0, we repeat this process m times to drive clusterings of k clusters by using each part in turn as the test set. \n",
    "\n",
    "###### The average of the quality measure is taken as the overall quality measure. We can then compare the overall quality measure with respect to different values of k, and find the number of clusters that best fits the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means with Optimal K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### init: Use \"k-means++\" instead of \"random\" to ensure centroids are initialized with some distance between them. In most cases, this will be an improvement over \"random\".\n",
    "\n",
    "###### n_clusters: The optimal number of clusters that was found in the previous step\n",
    "\n",
    "###### n_init: Increase the number of initializations to ensure a stable solution is found\n",
    "\n",
    "###### max_iter: Increase the number of iterations per initialization to ensure that k-means will converge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the optimal number of clusters\n",
    "n_clusters = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the k-means algorithm\n",
    "kmeans_kwargs = {\n",
    "    \"init\": \"k-means++\",\n",
    "    \"n_clusters\": n_clusters,\n",
    "    \"n_init\": 50,\n",
    "    \"max_iter\": 500,\n",
    "    \"random_state\": 42,\n",
    "}\n",
    "\n",
    "kmeans = KMeans(**kmeans_kwargs)\n",
    "\n",
    "# Fit the algorithm to the features\n",
    "kmeans.fit(X_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of iterations required to converge\n",
    "kmeans.n_iter_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final locations of the centroid\n",
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Internal Criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Sources\n",
    "\n",
    "###### https://www.cs.upc.edu/~bejar/URL/material/04-Validation.pdf\n",
    "###### https://nbviewer.jupyter.org/github/bejar/AMLTNotebooks/blob/master/Notebooks/10ClusterValidation.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### These indices do not require ground truth labels. They measure properties expected in a good clustering: compact groups and well-separated groups \n",
    "\n",
    "###### The indices are based on the model of the groups. We can use indices based on the attributes’ values measuring the properties of a good clustering. The indices are based on statistical properties of the attributes of the model: value distribution and distances distribution.\n",
    "\n",
    "###### Recent studies (Arbelatiz et al, 2013) have exhaustively tested internal indices, and some have a performance significativelly better than other. The study concludes that Silhouette, Davies-Bouldin and Calinski Harabasz perform well in a wide range of situations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Some of the indices correspond directly to the objective function optimizated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The lowest SSE value\n",
    "kmeans.inertia_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Silhouette Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Silhouette coefficient values range between -1 and 1. Larger numbers indicate that samples are closer to their clusters than they are to other clusters.\n",
    "\n",
    "###### A score of 1 denotes the best meaning that the data point o is very compact within the cluster to which it belongs and far away from the other clusters. Values near 0 denote overlapping clusters.\n",
    "\n",
    "###### The worst value is -1. When the silhouette coefficient value is negative, this means that, in expectation, o is closer to the objects in another cluster than to the objects in the same cluster as o. In many cases, this is a bad situation and should be avoided.\n",
    "\n",
    "###### In the scikit-learn implementation of the silhouette coefficient, the average silhouette coefficient of all the samples is summarized into one score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the silhouette score for the algorithm\n",
    "kmeans_silhouette = silhouette_score(\n",
    "    X_cluster, kmeans.labels_\n",
    ").round(2)\n",
    "\n",
    "kmeans_silhouette"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calinski-Harabasz Index      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calinski_harabasz_score(X_cluster, labels),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Davies-Bouldin Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "davies_bouldin_score(X_cluster, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Clustering Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Sources\n",
    "\n",
    "###### https://towardsdatascience.com/an-introduction-to-clustering-algorithms-in-python-123438574097\n",
    "###### https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68\n",
    "###### https://machinelearningmastery.com/clustering-algorithms-with-python/\n",
    "###### https://github.com/bejar/AMLTNotebooks/blob/master/Code/Validation/ValidationAuthors.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate k-means and dbscan algorithms\n",
    "dbscan = DBSCAN(eps=0.3)\n",
    "\n",
    "# Fit the algorithms to the features\n",
    "dbscan.fit(X_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the silhouette scores for the algorithm\n",
    "dbscan_silhouette = silhouette_score(\n",
    "    X_cluster, dbscan.labels_\n",
    ").round (2)\n",
    "\n",
    "dbscan_silhouette"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consensus Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Sources\n",
    "\n",
    "###### https://www.cs.upc.edu/~bejar/URL/material/06-Consensus.pdf\n",
    "###### https://nbviewer.jupyter.org/github/bejar/URLNotebooks/blob/master/Notebooks/12ConsensusClustering.ipynb\n",
    "###### https://learning.oreilly.com/library/view/hands-on-ensemble-learning/9781789612851/549666a5-7fe2-4ea9-867b-d5f8c640d28f.xhtml\n",
    "###### https://github.com/PacktPublishing/Hands-On-Ensemble-Learning-with-Python/tree/master/Chapter08"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Chosen Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Sources\n",
    "\n",
    "###### https://towardsdatascience.com/cluster-analysis-create-visualize-and-interpret-customer-segments-474e55d00ebb\n",
    "###### https://github.com/MaartenGr/CustomerSegmentation/blob/master/Customer%20Segmentation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the algorithm that produced the best results\n",
    "chosen_model = kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset utilized for clustering may have used more than 2 components\n",
    "# So, take the original processed matrix \"X\" and project it onto 2 dimensions\n",
    "# Then, attach the cluster labels to the final outputted dataframe\n",
    "\n",
    "df_pca = prepare_pca(2, X, chosen_model.labels_)\n",
    "df_tsne = prepare_tsne(2, X, chosen_model.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2d_v1 (df, model):\n",
    "    colors = plt.cm.Spectral(np.linspace(0, 1, len(df.labels.unique())))\n",
    "\n",
    "    for color, label in zip(colors, df.labels.unique()):\n",
    "    \n",
    "        tempdf = df[df.labels == label]\n",
    "        plt.scatter(tempdf.x, tempdf.y, c=color)\n",
    "    \n",
    "    plt.scatter(model.cluster_centers_[:,0], model.cluster_centers_[:, 1], c='r', s=500, alpha=0.7, )\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2d_v1(df_tsne, chosen_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset utilized for clustering may have more than 3 components\n",
    "# So, take the original processed matrix \"X\" and project it onto 3 dimensions\n",
    "# Then, attach the cluster labels to the final outputted dataframe\n",
    "\n",
    "df_pca = prepare_pca(3, X, chosen_model.labels_)\n",
    "df_tsne = prepare_tsne(3, X, chosen_model.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3d_v1(df, name='labels'):\n",
    "    fig = px.scatter_3d(df, x='x', y='y', z='z', color=name, opacity=0.5)\n",
    "    \n",
    "    fig.update_traces(marker=dict(size=3))\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_3d_v1(df_tsne)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3d_v2(df):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    plt.scatter(df[:, 0], df[:, 1], zs=df[:, 2], depthshade=False, c=df[:, 3], s=100)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_3d_v2(df_tsne)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D Animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_animation(df, name):\n",
    "    def update(num):\n",
    "        ax.view_init(200, num)\n",
    "\n",
    "    N=360\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(df['x'], df['y'], df['z'], c=df[\"labels\"],\n",
    "               s=6, depthshade=True, cmap='Paired')\n",
    "    ax.set_zlim(-15, 25)\n",
    "    ax.set_xlim(-20, 20)\n",
    "    plt.tight_layout()\n",
    "    ani = animation.FuncAnimation(fig, update, N, blit=False, interval=50)\n",
    "    #ani.save('{}.gif'.format(name), writer='imagemagick')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_animation(df_tsne, \"chosen_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Now that the clusters have been created, it would be nice to determine what makes each one unique. This will help with the understanding of the different observations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance Within Variables and Between Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### One way to see the differences between the clsuters is to take the average value of each cluster and visualize it\n",
    "\n",
    "###### The problem with this approach is that we simply have too many variables. Not all of them are likely to be important when creating the clusters. Instead, select the most important columns based on the following approach.\n",
    "\n",
    "###### Group datapoints by cluster and take the average. Then, calculate the standard deviation between those values for each variable. Variables with a higher standard deviation indicate that there are large differences between clusters and the variable might be important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go back the original dataframe that has all the features\n",
    "df_temp = df[df.columns[~df.columns.isin([\"GEOID\",\"GEO_ID\",\"NAME\"])]].copy()\n",
    "\n",
    "# Setting all variables between 0 and 1 in order to better visualize the results\n",
    "scaler = MinMaxScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df_temp))\n",
    "df_scaled.columns = df_temp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the labels of the chosen model\n",
    "df_temp[\"labels\"] = chosen_model.labels_\n",
    "df_scaled[\"labels\"] = chosen_model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate variables with largest differences (by standard deviation)\n",
    "# The higher the standard deviation in a variable based on average values for each cluster\n",
    "# The more likely that the variable is important when creating the cluster\n",
    "df_mean = df_scaled.loc[df_scaled.labels!=-1, :].groupby('labels').mean().reset_index()\n",
    "\n",
    "results = pd.DataFrame(columns=['Variable', 'Std'])\n",
    "\n",
    "for column in df_mean.columns[1:]:\n",
    "    results.loc[len(results), :] = [column, np.std(df_mean[column])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the number of columns to evaluate\n",
    "num_cols = 7\n",
    "\n",
    "# Put the contents of the top columns in a list\n",
    "selected_columns = list(results.sort_values('Std', ascending=False)\n",
    "                        .head(num_cols).Variable.values) + [\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot data\n",
    "tidy = df_scaled[selected_columns].melt(id_vars='labels')\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "sns.barplot(x='labels', y='value', hue='variable', data=tidy, palette='Set3')\n",
    "plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heatmap of Important Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Sources\n",
    "\n",
    "###### https://www.bigendiandata.com/2017-04-18-Jupyter_Customer360/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the scaled dataframe for the labels and important featues\n",
    "df_scaled_sub = df_scaled[selected_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataframe into cluster groups.\n",
    "# Then, compute the mean for all columns in every group\n",
    "df_grouped = df_scaled_sub.groupby([\"labels\"], sort=True).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the column labels in a list\n",
    "labels = list(df_scaled_sub.labels.unique())\n",
    "labels.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [go.Heatmap(z=df_grouped.values.tolist(), \n",
    "                   y=labels,\n",
    "                   x=list(df_grouped.columns),\n",
    "                   colorscale='Viridis')]\n",
    "\n",
    "plotly.offline.iplot(data, filename='pandas-heatmap')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Density Plots of Important Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Sources\n",
    "\n",
    "###### https://radiant-rstats.github.io/docs/multivariate/kclus.html\n",
    "###### https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.density.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(selected_columns[:-1])):\n",
    "\n",
    "    df_density = pd.DataFrame(df_scaled_sub[df_scaled_sub[\"labels\"]==0].iloc[:,i])\n",
    "    col = list(df_density.columns)[0]\n",
    "    df_density.rename(columns={col:\"0\"}, inplace=True)\n",
    "\n",
    "    for l in labels[1:]:\n",
    "        new_col = df_scaled_sub[df_scaled_sub[\"labels\"]==l].iloc[:,i]\n",
    "        df_density = pd.concat([df_density, new_col], axis=1, sort=False)\n",
    "        df_density.rename(columns={col:str(l)}, inplace=True)\n",
    "    \n",
    "    print(\"Density Plot for Feature: \" + selected_columns[i])\n",
    "    df_density.plot.kde()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Apply a Random Forest modelw ith the cluster labels as the target variable. This method can helpdetermine the features which are important in the generation of clusters.\n",
    "\n",
    "###### This method requires a bit more work since it's important to check the accuracy of the Random Forest model to accurately extract important features. Note that this step has been skipped below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "y = df_temp.iloc[:,-1]\n",
    "X = df_temp.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100).fit(X, y)\n",
    "selected_columns_rf = list(pd.DataFrame(np.array([clf.feature_importances_, X.columns]).T, columns=['Importance', 'Feature'])\n",
    "           .sort_values(\"Importance\", ascending=False)\n",
    "           .head(num_cols)\n",
    "           .Feature\n",
    "           .values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tidy = df_scaled[selected_columns_rf+['labels']].melt(id_vars='labels')\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "sns.barplot(x='labels', y='value', hue='variable', data=tidy, palette='Set3')\n",
    "plt.legend(loc='upper right')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
